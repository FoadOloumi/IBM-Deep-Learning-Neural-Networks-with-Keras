{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea86b895-2a85-4ee2-a83e-2c128955351d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  \n",
       "0          0.862735       -1.217079  \n",
       "1          1.055651       -1.217079  \n",
       "2         -0.526262       -2.239829  \n",
       "3         -0.526262       -2.239829  \n",
       "4          0.070492        0.647569  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###PART D###\n",
    "\n",
    "#Importing general libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Loading the concrete data from a downloaded file\n",
    "\n",
    "concrete_data_raw = pd.read_csv('concrete_data.csv')\n",
    "concrete_data_raw.head()\n",
    "concrete_data_raw.shape\n",
    "\n",
    "#Checking and cleanign the dataset\n",
    "\n",
    "concrete_data_raw.describe()\n",
    "concrete_data_raw.isnull().sum()\n",
    "\n",
    "#Age is not one of the predictors as per the assignment instructions, so it is deleted from the dataset.\n",
    "\n",
    "concrete_data = concrete_data_raw.drop('Age', axis = 1)\n",
    "\n",
    "#Separating predictors and target\n",
    "\n",
    "concrete_data_columns = concrete_data.columns \n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # All columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column\n",
    "\n",
    "#Normalizing data \n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "n_cols = predictors_norm.shape[1]\n",
    "predictors_norm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847cf23f-6ec4-420c-b9c7-7449b9fc9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definind the neural networks/models\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu', input_shape = (n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def regression_model_D():\n",
    "    # create model\n",
    "    model_D = Sequential()\n",
    "    model_D.add(Dense(10, activation = 'relu', input_shape = (n_cols,)))\n",
    "    model_D.add(Dense(10, activation = 'relu'))\n",
    "    model_D.add(Dense(10, activation = 'relu'))\n",
    "    model_D.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model_D.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    return model_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97ca149-c3bf-4adf-b6fa-f9220f0ba22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 7) (721,)\n",
      "(309, 7) (309,) (309, 1)\n",
      "(50,)\n",
      "[731.8887886521695, 350.7357217569465, 339.00102095765743, 288.1031016472974, 266.3492296777981, 323.4814301841568, 428.1613853485525, 657.0099973688434, 443.70443485582285, 298.7994634876812, 477.2999497336789, 354.3248051049902, 282.4091492030122, 361.70489640851474, 341.28526412859276, 286.8965736126161, 314.4194517202433, 479.19439778675365, 339.64023734056815, 466.09002479765917, 230.06786101362837, 395.18467413397957, 329.74254557949996, 299.9111535770928, 535.4086342969748, 351.7045607228072, 362.37855909409626, 345.13807402397623, 396.23089148586587, 379.79045952382893, 538.6897377143385, 287.20960455751845, 516.4461127574449, 352.5220648113162, 336.0571242514805, 539.5040914710677, 550.2653229643998, 349.86044957669617, 339.13559032386894, 258.8985115633903, 293.6455000273581, 332.22169797512095, 575.8645039105327, 279.47556716725825, 323.56714628377955, 337.7142439614184, 479.62941829892196, 330.33609352005533, 693.5947648744503, 368.14209601591205]\n",
      "\n",
      "The Mean of the mean squared errors is: 390.7767\n",
      "\n",
      "The Standard Deviation of the mean squared errors is: 113.7867\n"
     ]
    }
   ],
   "source": [
    "#Training the model, performing prediction and creating the list of MSEs for Part B\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Creating an empty list to store MSEs\n",
    "MSE_B = []\n",
    "\n",
    "#Performing a loop 50 times to train, test and evaluate the model, and update the MSEs\n",
    "for counter in range(50):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size = 0.3) #Splitting the dataset as per instructions.\n",
    "    model_B = regression_model()\n",
    "    model_B.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 50, verbose = 0) #Training the model as per instructions.\n",
    "    \n",
    "    y_pred = model_B.predict(X_test) #Using the model to predict the test set\n",
    "    MSE_B.append(mean_squared_error(y_test, y_pred)) #Updating mean squared error list with the new value\n",
    "    \n",
    "    counter += 1 \n",
    "\n",
    "print(X_train.shape, y_train.shape) #Confirming the successful data splitting operation\n",
    "print(X_test.shape, y_test.shape, y_pred.shape) #Confirming the successful data splitting operation\n",
    "print(np.shape(MSE_B)) #Confirming the number of MSEs collected \n",
    "print(MSE_B)\n",
    "\n",
    "print(\"\\nThe Mean of the mean squared errors is: %.4f\" % np.mean(MSE_B)) #Outputting the mean of MSEs \n",
    "print(\"\\nThe Standard Deviation of the mean squared errors is: %.4f\" % np.std(MSE_B)) #Outputting the mean of MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841e7115-a9dd-4e81-abcc-ae601b98e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 7) (721,)\n",
      "(309, 7) (309,) (309, 1)\n",
      "(50,)\n",
      "[164.15374165481416, 164.21875469733743, 150.5834231194652, 161.02422228175146, 155.88812981610528, 166.96948417705852, 154.99823355431906, 164.12158974711141, 171.13189473579055, 150.36919911382049, 161.74855751157176, 159.029578097831, 162.48633847806272, 164.06469520187292, 160.69412836998973, 147.79511927807985, 154.7796016722152, 164.97113013013512, 165.03394383472138, 158.7747540531403, 154.81096507971776, 166.84610064298005, 161.90670102233406, 156.95423406460918, 164.97328995955522, 155.3726728090532, 152.80055283255322, 155.89464539617657, 150.42151958843993, 173.38362754703968, 164.46889806887813, 165.61645869443393, 161.0057206715925, 158.67550718434651, 157.7240512656322, 158.6903306459428, 163.62404940062726, 166.60806207834747, 162.0569465550906, 162.77515915781024, 162.5722801749181, 161.809001708551, 162.76440055576822, 159.80127782109136, 154.76182736473297, 155.05041772231203, 166.54391430500274, 165.84023915049966, 161.6359621857259, 171.64381134127223]\n",
      "\n",
      "The Mean of the mean squared errors is: 160.7974\n",
      "\n",
      "The Standard Deviation of the mean squared errors is: 5.5841\n",
      "\n",
      "Adding more hidden layers to the NN, or making the network deeper, has decreased the Mean of the mean squared errors.\n",
      "\n",
      "Adding more hidden layers to the NN, or making the network deeper, has decreased the Standard Deviation of the mean squared errors.\n"
     ]
    }
   ],
   "source": [
    "#Training the model, performing prediction and creating the list of MSEs for Part D\n",
    "\n",
    "#Creating an empty list to store MSEs\n",
    "MSE_D = []\n",
    "\n",
    "#Performing a loop 50 times to train, test and evaluate the model, and update the MSEs\n",
    "for counter in range(50):\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size = 0.3) #Not splitting the dataset, using the same split as in Part B.\n",
    "    model_D = regression_model_D()\n",
    "    model_D.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 50, verbose = 0) #Training the model as per instructions.\n",
    "    \n",
    "    y_pred = model_D.predict(X_test) #Using the model to predict the test set\n",
    "    MSE_D.append(mean_squared_error(y_test, y_pred)) #Updating mean squared error list with the new value\n",
    "    \n",
    "    counter += 1 \n",
    "\n",
    "print(X_train.shape, y_train.shape) #Confirming the successful data splitting operation\n",
    "print(X_test.shape, y_test.shape, y_pred.shape) #Confirming the successful data splitting operation\n",
    "print(np.shape(MSE_D)) #Confirming the number of MSEs collected \n",
    "print(MSE_D)\n",
    "\n",
    "print(\"\\nThe Mean of the mean squared errors is: %.4f\" % np.mean(MSE_D)) #Outputting the mean of MSEs \n",
    "print(\"\\nThe Standard Deviation of the mean squared errors is: %.4f\" % np.std(MSE_D)) #Outputting the mean of MSEs\n",
    "\n",
    "if np.mean(MSE_D) > np.mean(MSE_B):\n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has increased the Mean of the mean squared errors.\")\n",
    "elif np.mean(MSE_D) == np.mean(MSE_B):\n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has had no effect on the Mean of the mean squared errors.\")\n",
    "else: \n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has decreased the Mean of the mean squared errors.\")\n",
    "    \n",
    "if np.std(MSE_D) > np.std(MSE_B):\n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has increased the Standard Deviation of the mean squared errors.\")\n",
    "elif np.std(MSE_D) == np.std(MSE_B):\n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has had no effect on the Standard Deviation of the mean squared errors.\")\n",
    "else: \n",
    "    print(\"\\nAdding more hidden layers to the NN, or making the network deeper, has decreased the Standard Deviation of the mean squared errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efeda3-0e61-4632-8852-e3e4214e461f",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "    For this part since the intent is to compare and test the change in neural network architecture by adding more hidden layers, \n",
    "    the dataset has not been split for Part D and the same dataset as in Part B has been used in order to keep the datasets consistent. \n",
    "    Increasing the number of hidden layers in the nueral network has significantly decreased the mean and STD of MSEs, even more so\n",
    "    than increasing the number of epochs in training.\n",
    "    \n",
    "Conclusion:\n",
    "    In order to achieve better results and minize the error, it is best to normalize the dataset, train the model longer (more epochs)\n",
    "    and train with a deep neural network with multiple hidden layers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
